{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类项目\n",
    "\n",
    "\n",
    "1. fasttest\n",
    "\n",
    "https://github.com/facebookresearch/fastText\n",
    "\n",
    "2. BERT\n",
    "\n",
    "https://github.com/Jiakui/awesome-bert\n",
    "\n",
    "3. GPT-2\n",
    "\n",
    "https://github.com/Morizeyao/GPT2-Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一步：安装fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> git clone https://github.com/facebookresearch/fastText.git\n",
    "\n",
    "> cd fastText\n",
    "\n",
    "> pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二步：准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  标签\n",
    "\n",
    "mapper_tag = {\n",
    "    '财经': 'Finance',\n",
    "    '彩票': 'Lottery',\n",
    "    '房产': 'Property',\n",
    "    '股票': 'Shares',\n",
    "    '家居': 'Furnishing',\n",
    "    '教育': 'Education',\n",
    "    '科技': 'Technology',\n",
    "    '社会': 'Sociology',\n",
    "    '时尚': 'Fashion',\n",
    "    '时政': 'Affairs',\n",
    "    '体育': 'Sports',\n",
    "    '星座': 'Constellation',\n",
    "    '游戏': 'Game',\n",
    "    '娱乐': 'Entertainment'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三步：数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有需要调用的库都写在前面\n",
    "\n",
    "import re\n",
    "from types import MethodType, FunctionType\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import fasttext.FastText as fasttext\n",
    "\n",
    "\n",
    "# 数据集路径写一个全局变量\n",
    "\n",
    "PATH = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\10786\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'上海 天然橡胶 期价 周三 再创 年内 新高 主力 合约 突破 21000 元 吨 关口'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据清洗\n",
    "\n",
    "def clean_txt(raw):\n",
    "    fil = re.compile(r\"[^0-9a-zA-Z\\u4e00-\\u9fa5]+\")\n",
    "    return fil.sub(' ', raw)\n",
    "\n",
    "def seg(sentence, sw, apply=None):\n",
    "    if isinstance(apply, FunctionType) or isinstance(apply, MethodType):\n",
    "        sentence = apply(sentence)\n",
    "    return ' '.join([i for i in jieba.cut(sentence) if i.strip() and i not in sw])\n",
    "\n",
    "def stop_words():\n",
    "    with open(PATH+'stopwords.txt', 'r', encoding='utf-8') as swf:\n",
    "        return [line.strip() for line in swf]\n",
    "\n",
    "    \n",
    "# 对某个sentence进行处理：\n",
    "content = '上海天然橡胶期价周三再创年内新高，主力合约突破21000元/吨重要关口。'\n",
    "res = seg(content.lower().replace('\\n', ''), stop_words(), apply=clean_txt)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 切割数据\n",
    "# 先将txt文件转换成csv文件，方便后面的计算\n",
    "\n",
    "class _MD(object):\n",
    "    mapper = {\n",
    "        str: '',\n",
    "        int: 0,\n",
    "        list: list,\n",
    "        dict: dict,\n",
    "        set: set,\n",
    "        bool: False,\n",
    "        float: .0\n",
    "    }\n",
    "\n",
    "    def __init__(self, obj, default=None):\n",
    "        self.dict = {}\n",
    "        assert obj in self.mapper, \\\n",
    "            'got a error type'\n",
    "        self.t = obj\n",
    "        if default is None:\n",
    "            return\n",
    "        assert isinstance(default, obj), \\\n",
    "            f'default ({default}) must be {obj}'\n",
    "        self.v = default\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.dict[key] = value\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item not in self.dict and hasattr(self, 'v'):\n",
    "            self.dict[item] = self.v\n",
    "            return self.v\n",
    "        elif item not in self.dict:\n",
    "            if callable(self.mapper[self.t]):\n",
    "                self.dict[item] = self.mapper[self.t]()\n",
    "            else:\n",
    "                self.dict[item] = self.mapper[self.t]\n",
    "            return self.dict[item]\n",
    "        return self.dict[item]\n",
    "\n",
    "\n",
    "def defaultdict(obj, default=None):\n",
    "    return _MD(obj, default)\n",
    "\n",
    "\n",
    "class TransformData(object):\n",
    "    def to_csv(self, handler, output, index=False):\n",
    "        dd = defaultdict(list)\n",
    "        for line in handler:\n",
    "            label, content = line.split(',', 1)\n",
    "            dd[label.strip('__label__').strip()].append(content.strip())\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for key in dd.dict:\n",
    "            col = pd.Series(dd[key], name=key)\n",
    "            df = pd.concat([df, col], axis=1)\n",
    "        return df.to_csv(output, index=index, encoding='utf-8')\n",
    "\n",
    "\n",
    "def split_train_test(source, auth_data=False):\n",
    "    if not auth_data:\n",
    "        train_proportion = 0.8\n",
    "    else:\n",
    "        train_proportion = 0.98\n",
    "\n",
    "    basename = source.rsplit('.', 1)[0]\n",
    "    train_file = basename + '_train.txt'\n",
    "    test_file = basename + '_test.txt'\n",
    "\n",
    "    handel = pd.read_csv(source, index_col=False, low_memory=False)\n",
    "    train_data_set = []\n",
    "    test_data_set = []\n",
    "    for head in list(handel.head()):\n",
    "        train_num = int(handel[head].dropna().__len__() * train_proportion)\n",
    "        sub_list = [f'__label__{head} , {item.strip()}\\n' for item in handel[head].dropna().tolist()]\n",
    "        train_data_set.extend(sub_list[:train_num])\n",
    "        test_data_set.extend(sub_list[train_num:])\n",
    "    shuffle(train_data_set)\n",
    "    shuffle(test_data_set)\n",
    "\n",
    "    with open(train_file, 'w', encoding='utf-8') as trainf,\\\n",
    "        open(test_file, 'w', encoding='utf-8') as testf:\n",
    "        for tds in train_data_set:\n",
    "            trainf.write(tds)\n",
    "        for i in test_data_set:\n",
    "            testf.write(i)\n",
    "\n",
    "    return train_file, test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='./datasets/data.txt' mode='r' encoding='utf8'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化成csv\n",
    "td = TransformData()\n",
    "handler = open(PATH+'data.txt',encoding='utf8')\n",
    "handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.to_csv(handler, PATH+'data.csv')\n",
    "handler.close()\n",
    "\n",
    "# 将csv文件切割，会生成两个文件（data_train.txt和data_test.txt）\n",
    "train_file, test_file = split_train_test(PATH+'data.csv', auth_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四步：训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(ipt=None, opt=None, model='', dim=100, epoch=5, lr=0.1, loss='softmax'):\n",
    "    np.set_printoptions(suppress=True)\n",
    "    if os.path.isfile(model):\n",
    "        classifier = fasttext.load_model(model)\n",
    "    else:\n",
    "        classifier = fasttext.train_supervised(ipt, label='__label__', dim=dim, epoch=epoch,\n",
    "                                         lr=lr, wordNgrams=2, loss=loss)\n",
    "        \"\"\"\n",
    "          训练一个监督模型, 返回一个模型对象\n",
    "\n",
    "          @param input:           训练数据文件路径\n",
    "          @param lr:              学习率\n",
    "          @param dim:             向量维度\n",
    "          @param ws:              cbow模型时使用\n",
    "          @param epoch:           次数\n",
    "          @param minCount:        词频阈值, 小于该值在初始化时会过滤掉\n",
    "          @param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉\n",
    "          @param minn:            构造subword时最小char个数\n",
    "          @param maxn:            构造subword时最大char个数\n",
    "          @param neg:             负采样\n",
    "          @param wordNgrams:      n-gram个数\n",
    "          @param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax\n",
    "          @param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量\n",
    "          @param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出\n",
    "          @param lrUpdateRate:    学习率更新\n",
    "          @param t:               负采样阈值\n",
    "          @param label:           类别前缀\n",
    "          @param verbose:         ??\n",
    "          @param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机\n",
    "          @return model object\n",
    "        \"\"\"\n",
    "        classifier.save_model(opt)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 0.9797775530839231, 0.9797775530839231)\n"
     ]
    }
   ],
   "source": [
    "# 调整以下参数观察分类结果\n",
    "\n",
    "dim = 200\n",
    "lr = 5\n",
    "epoch = 5\n",
    "\n",
    "model = f'data/data_test_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}.model'\n",
    "\n",
    "classifier = train_model(ipt=PATH+'data_train.txt',\n",
    "                         opt=model,\n",
    "                         model=model,\n",
    "                         dim=dim, epoch=epoch, lr=0.5\n",
    "                         )\n",
    "\n",
    "result = classifier.test(PATH+'data_test.txt')\n",
    "print(result)\n",
    "\n",
    "# 整体的结果为(测试数据量，precision，recall)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_precision_and_recall(file=PATH+'data_test.txt'):\n",
    "    precision = defaultdict(int, 1)\n",
    "    recall = defaultdict(int, 1)\n",
    "    total = defaultdict(int, 1)\n",
    "    with open(file,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            label, content = line.split(',', 1)\n",
    "            total[label.strip().strip('__label__')] += 1\n",
    "            labels2 = classifier.predict([seg(sentence=content.strip(), sw='', apply=clean_txt)])\n",
    "            pre_label, sim = labels2[0][0][0], labels2[1][0][0]\n",
    "            recall[pre_label.strip().strip('__label__')] += 1\n",
    "\n",
    "            if label.strip() == pre_label.strip():\n",
    "                precision[label.strip().strip('__label__')] += 1\n",
    "\n",
    "    print('precision', precision.dict)\n",
    "    print('recall', recall.dict)\n",
    "    print('total', total.dict)\n",
    "    for sub in precision.dict:\n",
    "        pre = precision[sub] / total[sub]\n",
    "        rec =  precision[sub] / recall[sub]\n",
    "        F1 = (2 * pre * rec) / (pre + rec)\n",
    "        print(f\"{sub.strip('__label__')}  precision: {str(pre)}  recall: {str(rec)}  F1: {str(F1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(source):\n",
    "    basename = source.rsplit('.', 1)[0]\n",
    "    csv_file = basename + '.csv'\n",
    "\n",
    "    td = TransformData()\n",
    "    handler = open(source,encoding='utf8')\n",
    "    td.to_csv(handler, csv_file)\n",
    "    handler.close()\n",
    "\n",
    "    train_file, test_file = split_train_test(csv_file)\n",
    "\n",
    "    dim = 100\n",
    "    lr = 5\n",
    "    epoch = 5\n",
    "    model = f'data/data_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}.model'\n",
    "\n",
    "    classifier = train_model(ipt=train_file,\n",
    "                             opt=model,\n",
    "                             model=model,\n",
    "                             dim=dim, epoch=epoch, lr=0.5\n",
    "                             )\n",
    "\n",
    "    result = classifier.test(test_file)\n",
    "    print(result)\n",
    "\n",
    "    cal_precision_and_recall(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9885, 0.973697521497218, 0.973697521497218)\n",
      "precision {'Entertainment': 1000, 'Technology': 998, 'Shares': 999, 'Furnishing': 998, 'Financ': 997, 'Affairs': 999, 'Sociology': 987, 'Gam': 869, 'Education': 998, 'Sports': 997}\n",
      "recall {'Entertainment': 1004, 'Technology': 998, 'Shares': 1003, 'Furnishing': 1001, 'Financ': 1000, 'Affairs': 1002, 'Sociology': 1001, 'Gam': 872, 'Education': 1016, 'Sports': 998}\n",
      "total {'Entertainment': 1001, 'Technology': 1001, 'Shares': 1001, 'Furnishing': 1001, 'Financ': 1001, 'Affairs': 1001, 'Sociology': 1001, 'Gam': 876, 'Education': 1001, 'Sports': 1001, 'Property': 11}\n",
      "Entertainment  precision: 0.999000999000999  recall: 0.9960159362549801  F1: 0.9975062344139651\n",
      "Technology  precision: 0.997002997002997  recall: 1.0  F1: 0.9984992496248124\n",
      "Shares  precision: 0.998001998001998  recall: 0.996011964107677  F1: 0.9970059880239521\n",
      "Furnishing  precision: 0.997002997002997  recall: 0.997002997002997  F1: 0.997002997002997\n",
      "Financ  precision: 0.996003996003996  recall: 0.997  F1: 0.9965017491254373\n",
      "Affairs  precision: 0.998001998001998  recall: 0.9970059880239521  F1: 0.9975037443834249\n",
      "Sociology  precision: 0.986013986013986  recall: 0.986013986013986  F1: 0.986013986013986\n",
      "Gam  precision: 0.9920091324200914  recall: 0.9965596330275229  F1: 0.9942791762013731\n",
      "Education  precision: 0.997002997002997  recall: 0.9822834645669292  F1: 0.9895884977689638\n",
      "Sports  precision: 0.996003996003996  recall: 0.998997995991984  F1: 0.9974987493746874\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(PATH+'data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
