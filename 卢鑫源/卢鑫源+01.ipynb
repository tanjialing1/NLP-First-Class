{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "fasttext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinerayLu/NN/blob/main/fasttext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pJ6RKFmq15c"
      },
      "source": [
        "# 文本分类项目\n",
        "\n",
        "\n",
        "1. fasttest\n",
        "\n",
        "https://github.com/facebookresearch/fastText\n",
        "\n",
        "2. BERT\n",
        "\n",
        "https://github.com/Jiakui/awesome-bert\n",
        "\n",
        "3. GPT-2\n",
        "\n",
        "https://github.com/Morizeyao/GPT2-Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUjJQrrq15e"
      },
      "source": [
        "# 第一步：安装fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGStAmmPq15f"
      },
      "source": [
        "> git clone https://github.com/facebookresearch/fastText.git\n",
        "\n",
        "> cd fastText\n",
        "\n",
        "> pip install ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U496D5AIsS4G",
        "outputId": "ecfa104f-726e-433d-e65a-88c2b1038c93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSxBwCZ_vRqa",
        "outputId": "c002a81d-ffe9-4137-f89e-821378ce8a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "y\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3025794 sha256=483a721e66c97031b737c462c02aa2329ff40f9522edc31b37217ea0f4784448\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akN5XuQeq15h"
      },
      "source": [
        "# 第二步：准备数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T07:18:38.487838Z",
          "start_time": "2020-10-27T07:18:38.437238Z"
        },
        "id": "UV2xUHjHq15i"
      },
      "source": [
        "#  标签\n",
        "\n",
        "mapper_tag = {\n",
        "    '财经': 'Finance',\n",
        "    '彩票': 'Lottery',\n",
        "    '房产': 'Property',\n",
        "    '股票': 'Shares',\n",
        "    '家居': 'Furnishing',\n",
        "    '教育': 'Education',\n",
        "    '科技': 'Technology',\n",
        "    '社会': 'Sociology',\n",
        "    '时尚': 'Fashion',\n",
        "    '时政': 'Affairs',\n",
        "    '体育': 'Sports',\n",
        "    '星座': 'Constellation',\n",
        "    '游戏': 'Game',\n",
        "    '娱乐': 'Entertainment'\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnKtYaGsq15r"
      },
      "source": [
        "# 第三步：数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T07:18:40.246936Z",
          "start_time": "2020-10-27T07:18:38.971689Z"
        },
        "id": "Unf_L6wfq15s"
      },
      "source": [
        "# 把所有需要调用的库都写在前面\n",
        "\n",
        "import re\n",
        "from types import MethodType, FunctionType\n",
        "import jieba\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import torch\n",
        "import os\n",
        "\n",
        "import fasttext.FastText as fasttext\n",
        "\n",
        "\n",
        "# 数据集路径写一个全局变量\n",
        "\n",
        "PATH = '/content/drive/My Drive/NLP第一次课/01_fasttext/datasets/'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T07:18:40.246936Z",
          "start_time": "2020-10-27T07:18:40.002Z"
        },
        "id": "FYhJ0_0Dq15y",
        "outputId": "806eeb14-268a-4622-c3e3-a0c106a79e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# 数据清洗\n",
        "\n",
        "def clean_txt(raw):\n",
        "    fil = re.compile(r\"[^0-9a-zA-Z\\u4e00-\\u9fa5]+\")\n",
        "    return fil.sub(' ', raw)\n",
        "\n",
        "def seg(sentence, sw, apply=None):\n",
        "    if isinstance(apply, FunctionType) or isinstance(apply, MethodType):\n",
        "        sentence = apply(sentence)\n",
        "    return ' '.join([i for i in jieba.cut(sentence) if i.strip() and i not in sw])\n",
        "\n",
        "def stop_words():\n",
        "    with open(PATH+'stopwords.txt', 'r', encoding='utf-8') as swf:\n",
        "        return [line.strip() for line in swf]\n",
        "\n",
        "    \n",
        "# 对某个sentence进行处理：\n",
        "content = '上海天然橡胶期价周三再创年内新高，主力合约突破21000元/吨重要关口。'\n",
        "res = seg(content.lower().replace('\\n', ''), stop_words(), apply=clean_txt)\n",
        "res"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.987 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'上海 天然橡胶 期价 周三 再创 年内 新高 主力 合约 突破 21000 元 吨 关口'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T07:18:40.716764Z",
          "start_time": "2020-10-27T07:18:40.686666Z"
        },
        "scrolled": true,
        "id": "WTU4nbrYq153"
      },
      "source": [
        "# 切割数据\n",
        "# 先将txt文件转换成csv文件，方便后面的计算\n",
        "\n",
        "class _MD(object):\n",
        "    mapper = {\n",
        "        str: '',\n",
        "        int: 0,\n",
        "        list: list,\n",
        "        dict: dict,\n",
        "        set: set,\n",
        "        bool: False,\n",
        "        float: .0\n",
        "    }\n",
        "\n",
        "    def __init__(self, obj, default=None):\n",
        "        self.dict = {}\n",
        "        assert obj in self.mapper, \\\n",
        "            'got a error type'\n",
        "        self.t = obj\n",
        "        if default is None:\n",
        "            return\n",
        "        assert isinstance(default, obj), \\\n",
        "            f'default ({default}) must be {obj}'\n",
        "        self.v = default\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self.dict[key] = value\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if item not in self.dict and hasattr(self, 'v'):\n",
        "            self.dict[item] = self.v\n",
        "            return self.v\n",
        "        elif item not in self.dict:\n",
        "            if callable(self.mapper[self.t]):\n",
        "                self.dict[item] = self.mapper[self.t]()\n",
        "            else:\n",
        "                self.dict[item] = self.mapper[self.t]\n",
        "            return self.dict[item]\n",
        "        return self.dict[item]\n",
        "\n",
        "\n",
        "def defaultdict(obj, default=None):\n",
        "    return _MD(obj, default)\n",
        "\n",
        "\n",
        "class TransformData(object):\n",
        "    def to_csv(self, handler, output, index=False):\n",
        "        dd = defaultdict(list)\n",
        "        for line in handler:\n",
        "            label, content = line.split(',', 1)\n",
        "            dd[label.strip('__label__').strip()].append(content.strip())\n",
        "\n",
        "        df = pd.DataFrame()\n",
        "        for key in dd.dict:\n",
        "            col = pd.Series(dd[key], name=key)\n",
        "            df = pd.concat([df, col], axis=1)\n",
        "        return df.to_csv(output, index=index, encoding='utf-8')\n",
        "\n",
        "\n",
        "def split_train_test(source, auth_data=False):\n",
        "    if not auth_data:\n",
        "        train_proportion = 0.8\n",
        "    else:\n",
        "        train_proportion = 0.98\n",
        "\n",
        "    basename = source.rsplit('.', 1)[0]\n",
        "    train_file = basename + '_train.txt'\n",
        "    test_file = basename + '_test.txt'\n",
        "\n",
        "    handel = pd.read_csv(source, index_col=False, low_memory=False)\n",
        "    train_data_set = []\n",
        "    test_data_set = []\n",
        "    for head in list(handel.head()):\n",
        "        train_num = int(handel[head].dropna().__len__() * train_proportion)\n",
        "        sub_list = [f'__label__{head} , {item.strip()}\\n' for item in handel[head].dropna().tolist()]\n",
        "        train_data_set.extend(sub_list[:train_num])\n",
        "        test_data_set.extend(sub_list[train_num:])\n",
        "    shuffle(train_data_set)\n",
        "    shuffle(test_data_set)\n",
        "\n",
        "    with open(train_file, 'w', encoding='utf-8') as trainf,\\\n",
        "        open(test_file, 'w', encoding='utf-8') as testf:\n",
        "        for tds in train_data_set:\n",
        "            trainf.write(tds)\n",
        "        for i in test_data_set:\n",
        "            testf.write(i)\n",
        "\n",
        "    return train_file, test_file"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-27T07:18:41.466306Z",
          "start_time": "2020-10-27T07:18:41.446407Z"
        },
        "id": "m7d3GHZkq16D"
      },
      "source": [
        "# 转化成csv\n",
        "td = TransformData()\n",
        "handler = open(PATH+'data.txt')\n",
        "td.to_csv(handler, PATH+'data.csv')\n",
        "handler.close()\n",
        "\n",
        "# 将csv文件切割，会生成两个文件（data_train.txt和data_test.txt）\n",
        "train_file, test_file = split_train_test(PATH+'data.csv', auth_data=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDONwCLoq16K"
      },
      "source": [
        "# 第四步：训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4LWBp4Fq16M"
      },
      "source": [
        "def train_model(ipt=None, opt=None, model='', dim=100, epoch=5, lr=0.1, loss='softmax'):\n",
        "    np.set_printoptions(suppress=True)\n",
        "    if os.path.isfile(model):\n",
        "        classifier = fasttext.load_model(model)\n",
        "    else:\n",
        "        classifier = fasttext.train_supervised(ipt, label='__label__', dim=dim, epoch=epoch,\n",
        "                                         lr=lr, wordNgrams=2, loss=loss)\n",
        "        \"\"\"\n",
        "          训练一个监督模型, 返回一个模型对象\n",
        "\n",
        "          @param input:           训练数据文件路径\n",
        "          @param lr:              学习率\n",
        "          @param dim:             向量维度\n",
        "          @param ws:              cbow模型时使用\n",
        "          @param epoch:           次数\n",
        "          @param minCount:        词频阈值, 小于该值在初始化时会过滤掉\n",
        "          @param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉\n",
        "          @param minn:            构造subword时最小char个数\n",
        "          @param maxn:            构造subword时最大char个数\n",
        "          @param neg:             负采样\n",
        "          @param wordNgrams:      n-gram个数\n",
        "          @param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax\n",
        "          @param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量\n",
        "          @param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出\n",
        "          @param lrUpdateRate:    学习率更新\n",
        "          @param t:               负采样阈值\n",
        "          @param label:           类别前缀\n",
        "          @param verbose:         ??\n",
        "          @param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机\n",
        "          @return model object\n",
        "        \"\"\"\n",
        "        classifier.save_model(opt)\n",
        "    return classifier\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS-JndNXq16P",
        "outputId": "f426b893-3286-48c4-ffaa-727d4dafdf91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 调整以下参数观察分类结果\n",
        "\n",
        "dim = 200\n",
        "lr = 5\n",
        "epoch = 5\n",
        "\n",
        "model = f'/content/drive/My Drive/NLP第一次课/01_fasttext/data/data_test_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}.model'\n",
        "\n",
        "classifier = train_model(ipt=PATH+'data_train.txt',\n",
        "                         opt=model,\n",
        "                         model=model,\n",
        "                         dim=dim, epoch=epoch, lr=0.5\n",
        "                         )\n",
        "\n",
        "result = classifier.test(PATH+'data_test.txt')\n",
        "print(result)\n",
        "\n",
        "# 整体的结果为(测试数据量，precision，recall)："
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(989, 0.9787664307381193, 0.9787664307381193)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zgy1_k_q16T"
      },
      "source": [
        "def cal_precision_and_recall(file=PATH+'data_test.txt'):\n",
        "    precision = defaultdict(int, 1)\n",
        "    recall = defaultdict(int, 1)\n",
        "    total = defaultdict(int, 1)\n",
        "    with open(file) as f:\n",
        "        for line in f:\n",
        "            label, content = line.split(',', 1)\n",
        "            total[label.strip().strip('__label__')] += 1\n",
        "            labels2 = classifier.predict([seg(sentence=content.strip(), sw='', apply=clean_txt)])\n",
        "            pre_label, sim = labels2[0][0][0], labels2[1][0][0]\n",
        "            recall[pre_label.strip().strip('__label__')] += 1\n",
        "\n",
        "            if label.strip() == pre_label.strip():\n",
        "                precision[label.strip().strip('__label__')] += 1\n",
        "\n",
        "    print('precision', precision.dict)\n",
        "    print('recall', recall.dict)\n",
        "    print('total', total.dict)\n",
        "    for sub in precision.dict:\n",
        "        pre = precision[sub] / total[sub]\n",
        "        rec =  precision[sub] / recall[sub]\n",
        "        F1 = (2 * pre * rec) / (pre + rec)\n",
        "        print(f\"{sub.strip('__label__')}  precision: {str(pre)}  recall: {str(rec)}  F1: {str(F1)}\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDTBI_AZq16X"
      },
      "source": [
        "def main(source):\n",
        "    basename = source.rsplit('.', 1)[0]\n",
        "    csv_file = basename + '.csv'\n",
        "\n",
        "    td = TransformData()\n",
        "    handler = open(source)\n",
        "    td.to_csv(handler, csv_file)\n",
        "    handler.close()\n",
        "\n",
        "    train_file, test_file = split_train_test(csv_file)\n",
        "\n",
        "    dim = 100\n",
        "    lr = 5\n",
        "    epoch = 5\n",
        "    model = f'/content/drive/My Drive/NLP第一次课/01_fasttext/data/data_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}.model'\n",
        "\n",
        "    classifier = train_model(ipt=train_file,\n",
        "                             opt=model,\n",
        "                             model=model,\n",
        "                             dim=dim, epoch=epoch, lr=0.5\n",
        "                             )\n",
        "\n",
        "    result = classifier.test(test_file)\n",
        "    print(result)\n",
        "\n",
        "    cal_precision_and_recall(test_file)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOHlPSAaq16b",
        "outputId": "d35c0f5b-bd3b-440b-d01e-03aec9b07e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main(PATH+'data.txt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9885, 0.9742033383915023, 0.9742033383915023)\n",
            "precision {'Education': 997, 'Technology': 998, 'Entertainment': 1001, 'Furnishing': 997, 'Sports': 996, 'Shares': 999, 'Financ': 998, 'Gam': 867, 'Sociology': 987, 'Affairs': 1000}\n",
            "recall {'Education': 1014, 'Technology': 999, 'Entertainment': 1006, 'Furnishing': 1003, 'Sports': 997, 'Shares': 1002, 'Financ': 1001, 'Gam': 870, 'Sociology': 1001, 'Affairs': 1002}\n",
            "total {'Education': 1001, 'Technology': 1001, 'Entertainment': 1001, 'Furnishing': 1001, 'Sports': 1001, 'Shares': 1001, 'Financ': 1001, 'Gam': 876, 'Sociology': 1001, 'Affairs': 1001, 'Property': 11}\n",
            "Education  precision: 0.996003996003996  recall: 0.9832347140039448  F1: 0.9895781637717121\n",
            "Technology  precision: 0.997002997002997  recall: 0.998998998998999  F1: 0.998\n",
            "Entertainment  precision: 1.0  recall: 0.9950298210735586  F1: 0.9975087194818136\n",
            "Furnishing  precision: 0.996003996003996  recall: 0.9940179461615155  F1: 0.9950099800399202\n",
            "Sports  precision: 0.995004995004995  recall: 0.9989969909729187  F1: 0.996996996996997\n",
            "Shares  precision: 0.998001998001998  recall: 0.9970059880239521  F1: 0.9975037443834249\n",
            "Financ  precision: 0.997002997002997  recall: 0.997002997002997  F1: 0.997002997002997\n",
            "Gam  precision: 0.9897260273972602  recall: 0.996551724137931  F1: 0.9931271477663229\n",
            "Sociology  precision: 0.986013986013986  recall: 0.986013986013986  F1: 0.986013986013986\n",
            "Affairs  precision: 0.999000999000999  recall: 0.998003992015968  F1: 0.9985022466300548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7mrMq08zhtL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
